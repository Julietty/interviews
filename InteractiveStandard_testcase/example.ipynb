{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "05dc0987-a971-4e1e-a268-f7a229b2acc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julia/venvs/python3.9/lib/python3.9/site-packages/umap/distances.py:1063: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/julia/venvs/python3.9/lib/python3.9/site-packages/umap/distances.py:1071: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/julia/venvs/python3.9/lib/python3.9/site-packages/umap/distances.py:1086: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "/Users/julia/venvs/python3.9/lib/python3.9/site-packages/umap/umap_.py:660: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n",
      "  @numba.jit()\n",
      "INFO:root:VERSION 2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import umap.umap_ as umap\n",
    "import IPython.display as display\n",
    "import ipywidgets as widgets\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from cycler import cycler\n",
    "from PIL import Image\n",
    "\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import ImageReadMode, read_image\n",
    "from torchvision import transforms\n",
    "from torchvision.models import efficientnet_b3, EfficientNet_B3_Weights\n",
    "\n",
    "import pytorch_metric_learning\n",
    "import pytorch_metric_learning.utils.logging_presets as LP\n",
    "from pytorch_metric_learning import losses, miners, samplers, testers, trainers\n",
    "from pytorch_metric_learning.utils.accuracy_calculator import AccuracyCalculator\n",
    "from pytorch_metric_learning.utils.inference import InferenceModel, MatchFinder\n",
    "from pytorch_metric_learning.distances import CosineSimilarity\n",
    "\n",
    "for handler in logging.root.handlers[:]:\n",
    "    logging.root.removeHandler(handler)\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.info(\"VERSION %s\" % pytorch_metric_learning.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "92879cd4-883d-41ea-a7d7-082c1384fa55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638e381c-4b48-44b1-9bd8-d22e145285d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6769f83-78b5-42e8-b89a-121c7b0f1712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0c9d93f-bcd5-4570-be10-c96f9128f2d0",
   "metadata": {},
   "source": [
    "## Описание датасета\n",
    "\n",
    "- Класс изображения - id внутри имени файла, он же по сути instance объекта, например, id товара вендора\n",
    "- Классов > половины длины датасета, то есть классическая классификация не пойдет, тк слой классификации будет слишком большой, а также если добавится новый класс в данные, то нужно переучивать модель\n",
    "- Данных на класс ~1.5 изображения\n",
    "\n",
    "## Подход\n",
    "- Выберем embedded подход для построения вероятности \"идентичности\" двух изображений, то есть будем учить модель предсказывать близкие эмбеддинги для похожих изображений и наоборот\n",
    "- Так как кластера у нас по сути это инстанс объекта, то размер кластера будет очень маленький, поэтому для нас важно разнести кластера на приличное расстояние друг от друга, для это будем использовать лосс с допуском (margin)\n",
    "- В результате получая на вход две картинки, мы производит препроцессинг, прогоняем через модель и получаем два эмбеддинга\n",
    "- Для получения финальной вероятности возьмем косинусное расстояние между эмбеддингами и переведем его из [-1, 1] в [0, 1], что будет равносильно распределению вероятностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80c7fec3-085e-4797-9a33-c8b5c011c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим на изображения\n",
    "image_paths = sorted(list(Path('test-task/clusters/').iterdir()))\n",
    "data = pd.read_csv('test-task/clusters.csv', index_col='Unnamed: 0')\n",
    "\n",
    "i = 0\n",
    "while i < 6:\n",
    "    # skip objects which contain less than 2 images\n",
    "    if str(image_paths[i]).split('_')[0] != str(image_paths[i + 1]).split('_')[0]:\n",
    "        i += 1\n",
    "        continue\n",
    "        \n",
    "    image1 = open(image_paths[i],'rb').read()\n",
    "    image2 = open(image_paths[i+1], 'rb').read()\n",
    "    \n",
    "    wi1 = widgets.Image(value=image1, format='jpg', width=300, height=400)\n",
    "    wi2 = widgets.Image(value=image2, format='jpg', width=300, height=400)\n",
    "    a = [wi1, wi2]\n",
    "    wid = widgets.HBox(a)\n",
    "    print(image_paths[i], image_paths[i + 1])\n",
    "    display.display(wid)\n",
    "\n",
    "    i += 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96950f6b-e433-427d-b793-c778c6cee0c0",
   "metadata": {},
   "source": [
    "## Параметры модели и обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61ad560b-b139-4322-bb03-3ba8c389be64",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "dataloader_num_workers = 2\n",
    "\n",
    "model_name ='EfficientNet_B3'\n",
    "image_size = (320, 300) # специфичный размер для EfficientNet_B3\n",
    "embedding_size = 256 \n",
    "batch_size = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c66298b-3f01-4e48-8365-63de22af7a03",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Создаем Дата класс"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e77541c-e49d-42b3-8774-f4b76bbd163e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_transforms() -> transforms.Compose:\n",
    "    return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.AutoAugment(transforms.AutoAugmentPolicy.IMAGENET),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "                \n",
    "            ]\n",
    "    )\n",
    "\n",
    "def get_valid_transforms() -> transforms.Compose:\n",
    "    return transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(size=image_size, interpolation=transforms.InterpolationMode.BILINEAR),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),                \n",
    "            ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d9cb6b0-4cb2-4387-b5c8-a22979f6a468",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    '''\n",
    "    Input:\n",
    "        data: DataFrame with image_path and label columns\n",
    "        transforms: image trainsformations, resize included\n",
    "    '''\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        image_dir: Path,\n",
    "        csv_layout: pd.DataFrame,\n",
    "        transforms: transforms.Compose = None,\n",
    "        return_labels=True,\n",
    "        verify_images=False\n",
    "    ):\n",
    "        self.image_dir = image_dir   \n",
    "        self.csv_layout = csv_layout\n",
    "        \n",
    "        self.image_to_label = None\n",
    "        self.label_to_images = None\n",
    "        self.csv_to_labels()\n",
    "        \n",
    "        self.images = list(self.csv_layout['file_name'])\n",
    "        self.images_paths = [Path(self.image_dir, image) for image in self.images]\n",
    "        self.labels = list(self.csv_layout['label'])\n",
    "        \n",
    "        if verify_images:\n",
    "            self.verify_images()\n",
    "        \n",
    "        self.augmentations = transforms\n",
    "        self.return_labels = return_labels\n",
    "        \n",
    "        assert self.image_to_label, \"There is no labels in data\" \n",
    "        \n",
    "    def csv_to_labels(self) -> None:\n",
    "        '''\n",
    "        Сonvert Pandas.DataFrame with claster - image columns\n",
    "        to dict image->label (i.e class of image)\n",
    "        '''\n",
    "        assert 'cluster_id' in self.csv_layout, \"cluster_id not in csv_layout\" \n",
    "        assert 'file_name' in self.csv_layout, \"file_name not in csv_layout\" \n",
    "        assert 'label' in self.csv_layout, \"label not in csv_layout\" \n",
    "        \n",
    "        self.image_to_label = defaultdict()\n",
    "        self.label_to_images = defaultdict(list)\n",
    "        \n",
    "        clusters = self.csv_layout['label']\n",
    "        images = self.csv_layout['file_name']\n",
    "        \n",
    "        for class_id, filename in zip(clusters, images):\n",
    "            self.image_to_label[filename] = class_id\n",
    "            self.label_to_images[class_id].append(filename)\n",
    "            \n",
    "            \n",
    "    def verify_images(self) -> None:\n",
    "        valid_images = 0\n",
    "        \n",
    "        logging.info(\"Start image validation\")\n",
    "        for image_path in tqdm(self.images_paths):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                valid_images += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f'corrupted image: {image_path}', e)\n",
    "                \n",
    "        logging.info(f\"Valid data: {valid_images}, {100 * valid_images/len(self.images_paths)}%\")\n",
    "        logging.info(f\"Corrupted data: {len(self.images_paths) - valid_images}, {100 * (1 - valid_images/len(self.images_paths))}%\")\n",
    "                \n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.images_paths)\n",
    "\n",
    "    def __getitem__(self, idx) -> Union[torch.tensor, List[torch.tensor]]:\n",
    "        \n",
    "        image_path = self.images_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.augmentations:\n",
    "            image = self.augmentations(image)\n",
    "        \n",
    "        if self.return_labels:\n",
    "            label = self.labels[idx]\n",
    "            return image, torch.tensor(label)\n",
    "        else:\n",
    "            return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "082809ca-7d10-45ac-9fa6-0f1d8b9a7d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# new_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a519d126-5997-40c8-bf7c-7aab474b14f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_cleaning = {\n",
    "    'f75f7082d62a44e7bfd373532877c9a6': 36,\n",
    "    '552656baa25848009b9eff3dfbedd15c': 10,\n",
    "    '96ebf9d10efa4804a5580f4d55e64d38': 16,\n",
    "    '3b9ba5ab3fce43c2bca84ee29e77c203': 39,\n",
    "    '726df8d4f65f4855849925746d01f0ae': 27,\n",
    "    '325cdb0bf6694d879a0f3518178feb60': 29,\n",
    "    'a1a79389069f4ddd86bfbdec7078ed02': 19,\n",
    "    '8d1c755d31a6484686c4a98ad45ab0e7': 29,\n",
    "    '499c8686053d47ca8395c4f5b7780bd0': 37,\n",
    "    '160dfcfe3c4e4623a3e9c360da87891c': 19,\n",
    "    '6b9c4d2192c74d82bbe02d52c5b1e007': 16,\n",
    "    '9fdbdc45cb7b4e53b0efaab769f7e224': 18,\n",
    "    '403446096ccf434f9e096b8291c40ac9': 18,\n",
    "    '23db675a000249498062844cdb31b76f': 18,\n",
    "    'd6367566a1dc445b87600b8c98ea5402': 18,\n",
    "    '980f8122f0db49cc94eef626826f3614': 22,\n",
    "    'd9d0792cefa74c55bdef834be6d34653': 20,\n",
    "    '670eb7df1cf54be09e18bc13496a50f1': 36,\n",
    "    '427ee587bd854410ada20b619b186f2b': 26,\n",
    "    '7bde72d406024ec0b7e72ca9e1edb311': 27,\n",
    "    '334b17efee804bfd82bb4189c3331ddb': 20,\n",
    "    'd94c01292d6f4fc19873234ccd85f091': 21,\n",
    "    '68b0cc47f2034ec98a3db6bf8b9df8e5': 13,\n",
    "    'afd68eec35824484b63a730ee8a19bef': 16,\n",
    "    '9e3d20f8c75f4dbea6085c514c4afb48': 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "409d0cf7-0af7-4edb-934d-78f666ebdbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes 26\n",
      "(1216, 3)\n",
      "(305, 3)\n"
     ]
    }
   ],
   "source": [
    "# Clean data\n",
    "image_paths = Path('test-task/clusters/')\n",
    "data = pd.read_csv('test-task/clusters.csv', index_col='Unnamed: 0')\n",
    "\n",
    "new_clusters = defaultdict(list)\n",
    "\n",
    "for cluster, thresh_idx in data_cleaning.items():\n",
    "    images = list(data[data.cluster_id == cluster].file_name)\n",
    "    new_clusters[cluster] = images[:thresh_idx]\n",
    "    new_clusters['trash'].extend(images[thresh_idx:])\n",
    "    \n",
    "new_data = []\n",
    "for cluster_id, images in new_clusters.items():\n",
    "    new_data.extend([[cluster_id, image] for image in images])\n",
    "\n",
    "data = pd.DataFrame(new_data, columns = ['cluster_id', 'file_name'])\n",
    "num_classes = len(data.cluster_id.unique())\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "data['label'] = encoder.fit_transform(data['cluster_id'])\n",
    "\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split(data[['cluster_id', 'file_name']], \n",
    "                                                              data['label'], stratify=data['label'], \n",
    "                                                              test_size=0.2, random_state=42)\n",
    "train_df = pd.concat([X_train_df, y_train_df], axis=1)\n",
    "val_df = pd.concat([X_val_df, y_val_df], axis=1)\n",
    "\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "val_df.reset_index(drop=True, inplace=True)\n",
    "# test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print('num_classes', num_classes)\n",
    "print(train_df.shape)\n",
    "print(val_df.shape)\n",
    "# print(test_df.shape)\n",
    "\n",
    "train_dataset = ImageDataset(image_dir=image_paths, csv_layout=train_df, transforms=get_train_transforms())\n",
    "val_dataset = ImageDataset(image_dir=image_paths, csv_layout=val_df, transforms=get_valid_transforms())\n",
    "# test_dataset = ImageDataset(image_dir=image_paths, csv_layout=test_df, transforms=get_valid_transforms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2bccfeb6-1b5d-419b-9d4a-00570f004448",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'difference'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdifference\u001b[49m(\u001b[38;5;28mset\u001b[39m(val_dataset\u001b[38;5;241m.\u001b[39mlabels))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'difference'"
     ]
    }
   ],
   "source": [
    "train_df.label.unique().difference(set(val_dataset.labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ee49d8-4fed-4259-a675-113b4c2f15a2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Инициилизирует необходимое и обучим модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9133d00-f998-4b4e-9e0a-551e42629583",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bb29de8-4c8a-4faa-9186-b3e2f56261ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Возьмем Effientnet в качестве базовой модели и заметим слой классификации на тождественное преобразование\n",
    "# То есть на выходе будем получать сырые фичи из бэкбона\n",
    "trunk = efficientnet_b3(weights=EfficientNet_B3_Weights.DEFAULT)\n",
    "trunk_output_size = trunk.classifier[1].in_features\n",
    "trunk.classifier = nn.Identity()\n",
    "trunk = torch.nn.DataParallel(trunk.to(device))\n",
    "\n",
    "# Сделаем простенький эмбеддер, который будет преобразовывать фичи из бэкбона в наше латентное пространство\n",
    "# Приемлемое качество далось на размерности пространства (финального эмбеддинга) = 512\n",
    "simple_embedder = nn.Sequential(nn.Linear(trunk_output_size, embedding_size))\n",
    "embedder = torch.nn.DataParallel(simple_embedder.to(device))\n",
    "\n",
    "# Set optimizers\n",
    "trunk_optimizer = torch.optim.Adam(trunk.parameters(), lr=0.00001, weight_decay=0.0001)\n",
    "embedder_optimizer = torch.optim.Adam(\n",
    "    embedder.parameters(), lr=0.0001, weight_decay=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdffcf65-ea81-48c6-8923-19af58641bc5",
   "metadata": {},
   "source": [
    "В данной реализации задачи я взяла библиотеку `pytorch_metric_learning`, в ней реализованы интересные фичи для визуализации нашего пространства.\n",
    "В остальном библиотека содержит обертки над стандартными классами `pytorch`, а также популярные лоссы для метрического обучения.\n",
    "\n",
    "\n",
    "Я взяла `TripletMarginLoss`, так как это один из базовых лоссов для разделения классов + он имеет margin, который важен в нашем случае очень маленького размера класса. Более сложные лоссы вроде cosface/arcface использовать в данной задаче смысла не вижу, к тому же домен фотографий в нашем случае сильно отличается от домена лиц, в котором решаются специфические задачи с помощью arcfase лосса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aadfbce8-6f2c-467e-b6dc-25176a8eaaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Лосс функция\n",
    "loss = losses.ArcFaceLoss(num_classes=num_classes, embedding_size=embedding_size, margin=0.3, scale=5)\n",
    "\n",
    "# Функция создания пар во время обучения\n",
    "miner = miners.MultiSimilarityMiner(epsilon=0.1)\n",
    "\n",
    "# Сэмплер данных\n",
    "sampler = samplers.MPerClassSampler(\n",
    "    train_dataset.labels, m=5, length_before_new_iter=len(train_dataset)\n",
    ")\n",
    "\n",
    "# Формируем финальные параметры для тренировки\n",
    "models = {\n",
    "    \"trunk\": trunk, \n",
    "    \"embedder\": embedder\n",
    "}\n",
    "optimizers = {\n",
    "    \"trunk_optimizer\": trunk_optimizer,\n",
    "    \"embedder_optimizer\": embedder_optimizer,\n",
    "}\n",
    "\n",
    "loss_funcs = {\"metric_loss\": loss}\n",
    "# mining_funcs = {\"tuple_miner\": miner}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702c1b78-b4cc-459c-b376-fbaaba01e1ff",
   "metadata": {},
   "source": [
    "### Вспомогательные методы для визуализации пространства, а также тестирование и сохранение лучших весов модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4c1ca17-dca1-4022-b283-a0af6b818697",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_metric_learning.utils import logging_presets\n",
    "\n",
    "record_keeper, _, _ = logging_presets.get_record_keeper(\"logs\", \"tensorboard\")\n",
    "hooks = logging_presets.get_hook_container(record_keeper)\n",
    "dataset_dict = {\"val\": val_dataset}\n",
    "model_folder = \"saved_models\"\n",
    "\n",
    "\n",
    "def visualizer_hook(umapper, umap_embeddings, labels, split_name, keyname, *args):\n",
    "    logging.info(\n",
    "        \"UMAP plot for the {} split and label set {}\".format(split_name, keyname)\n",
    "    )\n",
    "    label_set = np.unique(labels)\n",
    "    num_classes = len(label_set)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.gca().set_prop_cycle(\n",
    "        cycler(\n",
    "            \"color\", [plt.cm.nipy_spectral(i) for i in np.linspace(0, 0.9, num_classes)]\n",
    "        )\n",
    "    )\n",
    "    for i in range(num_classes):\n",
    "        idx = labels == label_set[i]\n",
    "        plt.plot(umap_embeddings[idx, 0], umap_embeddings[idx, 1], \".\", markersize=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Тестировщик на этапе валидации модельки, после подсчета метрик строится векторное пространство\n",
    "tester = testers.GlobalEmbeddingSpaceTester(\n",
    "    end_of_testing_hook=hooks.end_of_testing_hook,\n",
    "    visualizer=umap.UMAP(),\n",
    "    visualizer_hook=visualizer_hook,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    accuracy_calculator=AccuracyCalculator(k=\"max_bin_count\"),\n",
    ")\n",
    "\n",
    "end_of_epoch_hook = hooks.end_of_epoch_hook(\n",
    "    tester, dataset_dict, model_folder, test_interval=1, patience=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4425a74d-64b1-4bd4-b696-faf42b2c083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = trainers.MetricLossOnly(\n",
    "    models=models,\n",
    "    optimizers=optimizers,\n",
    "    batch_size=batch_size,\n",
    "    loss_funcs=loss_funcs,\n",
    "    dataset=train_dataset,\n",
    "    mining_funcs={},\n",
    "    sampler=sampler,\n",
    "    dataloader_num_workers=0,\n",
    "    end_of_iteration_hook=hooks.end_of_iteration_hook,\n",
    "    end_of_epoch_hook=end_of_epoch_hook,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0970c0a5-24b0-45ca-8345-f68fbf3c1d68",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PML:Initializing dataloader\n",
      "INFO:PML:Initializing dataloader iterator\n",
      "INFO:PML:Done creating dataloader iterator\n",
      "INFO:PML:TRAINING EPOCH 1\n",
      "total_loss=3.26221:   1%|▉                                                                                     | 1/97 [00:20<32:50, 20.52s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/trainers/base_trainer.py:87\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[0;34m(self, start_epoch, num_epochs)\u001b[0m\n\u001b[1;32m     85\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm\u001b[38;5;241m.\u001b[39mtqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterations_per_epoch))\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_and_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend_of_iteration_hook(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m     89\u001b[0m     pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss=\u001b[39m\u001b[38;5;132;01m%.5f\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/trainers/base_trainer.py:115\u001b[0m, in \u001b[0;36mBaseTrainer.forward_and_backward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdate_loss_weights()\n\u001b[0;32m--> 115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_tracker\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_weights)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/trainers/metric_loss_only.py:7\u001b[0m, in \u001b[0;36mMetricLossOnly.calculate_loss\u001b[0;34m(self, curr_batch)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_loss\u001b[39m(\u001b[38;5;28mself\u001b[39m, curr_batch):\n\u001b[1;32m      6\u001b[0m     data, labels \u001b[38;5;241m=\u001b[39m curr_batch\n\u001b[0;32m----> 7\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     indices_tuple \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_mine_embeddings(embeddings, labels)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaybe_get_metric_loss(\n\u001b[1;32m     10\u001b[0m         embeddings, labels, indices_tuple\n\u001b[1;32m     11\u001b[0m     )\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/trainers/base_trainer.py:142\u001b[0m, in \u001b[0;36mBaseTrainer.compute_embeddings\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_embeddings\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m--> 142\u001b[0m     trunk_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_trunk_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_final_embeddings(trunk_output)\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/trainers/base_trainer.py:151\u001b[0m, in \u001b[0;36mBaseTrainer.get_trunk_output\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trunk_output\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[1;32m    150\u001b[0m     data \u001b[38;5;241m=\u001b[39m c_f\u001b[38;5;241m.\u001b[39mto_device(data, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_device, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrunk\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/parallel/data_parallel.py:153\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataParallel.forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids:\n\u001b[0;32m--> 153\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m chain(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39mbuffers()):\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msrc_device_obj:\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torchvision/models/efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torchvision/models/efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torchvision/models/efficientnet.py:164\u001b[0m, in \u001b[0;36mMBConv.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 164\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_res_connect:\n\u001b[1;32m    166\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstochastic_depth(result)\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    457\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    458\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 459\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train(num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec3360-ab5f-41d3-ad25-a1270884505d",
   "metadata": {},
   "source": [
    "### Заметки по обучению\n",
    "\n",
    "1. Обучение сходится за 3 эпохи с хорошими целевыми метриками (показаны ниже) на отложенной выборке, это значит, что модель достаточно хорошо обобщается на наших данных без переобучения\n",
    "2. Анализируя векторное пространство можно заметить, что вектора почти не формируются в плотные кластера, что верно описывает наши разреженные данные. Пространство достаточно равномерное. Конечно, из-за маленького размера кластера не представляется возможным оценить false positive эмбеддинги, то есть неверно сближенные."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e2382-a76f-481c-83c5-8b403ca24218",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Искусственно создадим тестовую выборку и посчитаем precision, accuracy, recall, f1-score\n",
    "\n",
    "Для этого возьмем отложенную тестовую выборку и создадим из нее две части. В сумме размер теста сделаем 1000 пар:\n",
    "1. Пары, где картинки из одного класса, попавшие при train_test разбиении в тестовую часть. Их фиксированное число, пусть будет `n`\n",
    "2. Пары, где картинки из разного класса. Рандомно выберем номера instance для каждой пары. Таких пар будет `1000 - n`\n",
    "\n",
    "Очевидно, что при таком подходе создании пар непохожих изображений качество может немного меняться, однако если модель устойчива, то качество меняется не стат. значимо. Для бОльшей уверенности в результатах можно генерировать тестовую выборку несколько раз и усреднять результаты. Я проверяла такой поход на маленьком тест-сете, чтобы убедиться в верности своих рассуждений, однако в итоговых результатах я привожу значения целевых метриках на одном полноценном прогоне в связи с временными ограничениями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a305b6b-8e04-4687-892f-ddc8bd82d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix\n",
    "\n",
    "from torch.nn import functional\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "280945b7-b86f-4450-b869-cf3c5a4bf2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_samples_from_class(dataset: ImageDataset, idx: int) -> None:\n",
    "    images = dataset.label_to_images[idx]\n",
    "    image_path = dataset.image_dir\n",
    "    pairs = np.array_split(images, int(len(images) / 2))\n",
    "\n",
    "    for pair in pairs:\n",
    "        path1 = os.path.join(image_path, pair[0])\n",
    "        path2 = os.path.join(image_path, pair[1])\n",
    "        \n",
    "        image1 = open(path1,'rb').read()\n",
    "        image2 = open(path2, 'rb').read()\n",
    "\n",
    "        wi1 = widgets.Image(value=image1, format='jpg', width=300, height=400)\n",
    "        wi2 = widgets.Image(value=image2, format='jpg', width=300, height=400)\n",
    "        a = [wi1, wi2]\n",
    "        wid = widgets.HBox(a)\n",
    "        print(pair[0], pair[1])\n",
    "        display.display(wid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cae03493-dda9-4b98-bbc5-e295a3161ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.images_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "a053c049-06a0-46ed-be05-9ebce1800424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.TextIOWrapper name='cyprus_testcase/CYPRUS_DATA/clusters/a834306344194287b0311ed05accdbcd.jpg' mode='r' encoding='UTF-8'>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "open(val_dataset.images_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ee7f8a8d-afa6-4689-9288-a47e07a67fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_two_images_by_idx(dataset, idx1, idx2):\n",
    "    image_path = dataset.image_dir\n",
    "    \n",
    "    path1 = dataset.images_paths[idx1]\n",
    "    path2 = dataset.images_paths[idx2]\n",
    "\n",
    "    image1 = open(path1,'rb').read()\n",
    "    image2 = open(path2, 'rb').read()\n",
    "\n",
    "    wi1 = widgets.Image(value=image1, format='jpg', width=300, height=400)\n",
    "    wi2 = widgets.Image(value=image2, format='jpg', width=300, height=400)\n",
    "    a = [wi1, wi2]\n",
    "    wid = widgets.HBox(a)\n",
    "    print(path1, path2)\n",
    "    display.display(wid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "ac4ad44b-cc6d-4977-8f48-18e2357546ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cyprus_testcase/CYPRUS_DATA/clusters/a834306344194287b0311ed05accdbcd.jpg cyprus_testcase/CYPRUS_DATA/clusters/1eec91c1ab8b4498a83afd88e65519dd.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f9be0d43374ca79d9bd14fa396b4b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x02HExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_two_images_by_idx(val_dataset, 0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "191b14a0-d5de-497b-aea9-a98f86b2f9aa",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b46d0850ae334f198cdb6d39c61ae939.jpg b18d3749990c4a26a04c9d280deea40d.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec8646606582477b99a0fdb7d91b8903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1,\\xa5Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3f18414c699e4967aa5814560f99fe1b.jpg 258c948b871049579015be0846eec208.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171df60a213e4df0aa88706e5c4e8732",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1%\\xd7Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13a11a4b24614be7b7cfde398ffcd9e5.jpg 0dfa00218e0e44cb904efcb1bf76faae.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66172ead2f84b488300daaca6cd393d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1-\\xfaExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13adbaf02dc44e31a7272df6256a7e7f.jpg 0de1d340f8674816a03e05601c9978f4.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fdcb938bc334aaa9340db9b715d4049",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1*4Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f253dab885824dbca2cdb3cc340bd5e1.jpg e01d8019aa9a4e47a6b29051668cb272.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3a0af24645141abb4509413e4a9072c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe10>Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4cc00b7c1b2746b7b4a5f9fd93c80aba.jpg 13ecd1dffb524ae591038c835e4ad1c2.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658cee891ebb4f4d8f27196a9a13ab75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1!oExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\x00…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b7e8ac91ff6449cfb6e013ca4140f482.jpg 0ff4bd2c53554c44a185de3eb109b82e.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b42186504b4eec830ae6c2ad014009",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1.\\xadExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c1999090fbba4dfa8e58b0cc6a0a7acf.jpg 33105be04aee4651a7d7d4141b9a302f.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222ba4397e5b4acbb28fcade77bc1424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1/\\xd7Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e969c58391cc441283b03d9b9e9b6f9a.jpg 0c8decd5c8eb420dbfc9ad25b64deae0.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95876d7e5fb6490c8c47373971a3c6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe19\\xfbExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\x08\\x01\\x0f\\x00\\x02\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4bde3487693343aaa93bdf592ebd723d.jpg 12bc71d49680410daf0017281d8bd004.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde99eaed5094c88a90d53e1e9ab54c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00H\\x00H\\x00\\x00\\xff\\xe1\\x0f\\xb4Exi…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aeaa69a1b15e48b8ac64e25501622f19.jpg b00b320482664f3592754d09c0675d52.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4479fe5c1f824e2cb9d7de620281b7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53bd182a2b32475d8f29aa5cc5eddaef.jpg d3994979816a41068d26c4d8326c4918.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6b52d130f644e5af8090a28dae1e41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x01\\x8bExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8413402c52d743049f24c48d9c12e391.jpg b057dea9038b4709a4dd65d9837e47af.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84cf33a925154fd195955c08463be1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x01\\xd2Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b37277ef7df443d7bab8c6a246b54738.jpg 8ad1ac68a72e4596b8fbaccd0a896539.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f54c4398c74a2f9ffe541c45405941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x01\\xd6Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dcb4f599afbf4ccea8586d8f46a74b7b.jpg cce54877a10147df9b12d63eb8f113d1.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff9df875410646f4b2262a815e429cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acab1faa1b794a758153b68329b249df.jpg 867bb0a2273f43df91f4d028225073cd.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25efb0ef061e4fdaa4b85f5913f47863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "590e0fa9daf848748ad133ace54e3d5f.jpg 11af3e23be31429497a6b9df4b74002c.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "102e6b9fa16b4e59a6828cb27a05042e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x02\\xbeExif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d3ff2ebdd50e42e5ac00707215e9d58e.jpg 7e0865158b9d42499b5067960642086f.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92c82c0e65604ec38098155aab1110a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "479cdf8a9e9f4a648fb1ca04356e8c38.jpg 4943884054a8444c83013d7b1549cf82.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d96d3bc65d48d09342a37b81ae6b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x01\\xe4Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eea58c08c02248c1b561d7044776783a.jpg 2957fe92d6494f05a961afc60974dc35.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a36cd30f719c4a109f5e9bf9b691e6a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eec9b9a2ca0c499582233e20f2c71702.jpg 5a9de0d80666441da81b6a43ec9badcb.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c152e2c3fb614b27a9d708cb1131960b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "da5f0111eb4746f0acf99564c06da436.jpg 05e91c2dde344748b2be2839a7e78727.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33eb097722f4e97847f19dbced29582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x01\\xa8Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d9ffc8d1468a43018122e24c708a9dcf.jpg 93bb8bf752c1468eb55e65a3f1bf478b.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3ffed86c5cc4521b9d9ffeabcb73820",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d3c75a1e588c461c93963c94dd4ae83a.jpg c1f079e2cb314451a662cc5f221c5fce.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd742f788dd1416fb6f4a60e1e212a5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fed61fda6a094e529530721c7cd13316.jpg cb38933c074f4701b8da32c51ad49189.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2fe8a0fc8049b19a6ce706a2375930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d7596efc1adb4625aa454c2ab090139f.jpg f4f90dd94405430dae3c35e6cda27c66.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a0d6b313674837a89d179d78aa3c2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ead79824119a498a98b1952687851dc4.jpg c1e5f24e7b46479c9550ee9b6acee582.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbcb20dadcb34db097ca91f4bf0d7853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7f0300ba8ef343b2ac420299c11df922.jpg d6bb7edc5fe8446bae64217de5da6a14.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6f16a97e4054c10a000ae406b837138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0d7e14ed9f4e4466abf1c6b64280ca45.jpg 1dd1ccc7d1d44d3f9d1b221f693714a8.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69332602964c468db8b53330c8c664bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe1\\x01\\xc5Exif\\x00\\x00MM\\x00*\\x00\\x00\\x00\\x08\\x00\\t\\x01\\x0e\\x00\\x02…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2af9acaa67cb418e94c0c750199844dc.jpg 3c2ad926934845f789649e2311ce3dbb.jpg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93c2a37740b45168ea8610cc8708d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\xff\\xdb\\x00C…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_samples_from_class(val_dataset, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fa4b74d9-af8a-4970-bbab-28515ce9e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_samples_from_class(val_dataset, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f77b646-4278-4c31-a659-d9fbd2b21ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим экземляр инференса, по сути это просто обертка над двумя сетками, которая возвращает net2(net1(tensor))\n",
    "inference_model = InferenceModel(\n",
    "    trunk=trunk,\n",
    "    embedder=embedder,\n",
    "    normalize_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce3f5678-7fb3-4fa5-b8b1-5a60d2f71cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcea050-7baa-400a-90be-1f5cdc77dd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кол-во пар идентичных изображений: 305\n",
      "Кол-во пар различных изображений: 195\n",
      "Кол-во пар идентичных изображений + различных изображений: 500\n"
     ]
    }
   ],
   "source": [
    "# Cоберем все существующие пары, которые попали в тест\n",
    "n_samples = 500\n",
    "\n",
    "pairs = []\n",
    "targets = [0] * n_samples\n",
    "dupli = val_df.label[val_df.label.duplicated() == True]\n",
    "dupli_labels = list(set(dupli))\n",
    "\n",
    "for l in dupli_labels:\n",
    "    idxs = list(val_df.label[val_df.label == l].index)\n",
    "    idxs_full = [(idxs[i], idxs[j]) for i in range(len(idxs)) for j in range(i + 1, len(idxs))]\n",
    "\n",
    "    pairs.extend(random.choices(idxs_full, k=len(idxs)))\n",
    "    \n",
    "targets[:len(pairs)] = [1] * len(pairs)\n",
    "    \n",
    "print('Кол-во пар идентичных изображений:', len(pairs))\n",
    "print('Кол-во пар различных изображений:', n_samples - len(pairs))\n",
    "\n",
    "\n",
    "# соберем разные изображения, т.е. instance/label разные\n",
    "n_labels = len(set(val_df.label))\n",
    "\n",
    "for _ in range(n_samples - len(pairs)):\n",
    "    idx1, idx2 = random.choice(val_df.label), random.choice(val_df.label)\n",
    "    while idx1 == idx2 or val_df.label[idx1] == val_df.label[idx2]:\n",
    "        idx1, idx2 = random.choice(val_df.label), random.choice(val_df.label)\n",
    "        \n",
    "    pairs.append([idx1, idx2])\n",
    "    \n",
    "print('Кол-во пар идентичных изображений + различных изображений:', len(pairs))\n",
    "assert len(pairs) == len(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "736d8157-5fdf-483a-875a-c1bfaedb44f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# distances = []\n",
    "# for idx1, idx2 in tqdm(pairs):\n",
    "#     _, dist = is_match(val_dataset, idx1, idx2, return_dist=True)\n",
    "#     distances.append(dist)\n",
    "    \n",
    "# for dist, (idx1, idx2) in zip(distances, pairs):\n",
    "#     if dist > 0.7:\n",
    "#         print(dist, val_dataset.labels[idx1], val_dataset.labels[idx2])\n",
    "#         show_two_images_by_idx(val_dataset, idx1, idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b28d26cb-70c7-4b4e-bb3e-9536e0815f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[6, 7],\n",
       " [0, 9],\n",
       " [14, 15],\n",
       " [14, 15],\n",
       " [12, 17],\n",
       " [6, 7],\n",
       " [0, 9],\n",
       " [12, 17],\n",
       " [22, 24],\n",
       " [11, 25]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairs[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "563cc295-412b-49ef-93d8-a1d1b204106c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87b7c0e5-790f-44a2-87c8-e4cf6c57bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Незамысловая функция сравнения двух эмбеддингов\n",
    "# Для перебора порогов можно получить также расстояние\n",
    "def is_match(dataset:ImageDataset, idx1, idx2, threshold=0.9, return_dist=False) -> Union[bool, Tuple[bool, float]]:\n",
    "    embed1 = inference_model.get_embeddings(dataset[idx1][0].unsqueeze(0))\n",
    "    embed2 = inference_model.get_embeddings(dataset[idx2][0].unsqueeze(0))\n",
    "    \n",
    "    # cos ~ [-1, 1] -> (cos + 1) / 2 ~ (0, 1) ~ вероятность идентичности \n",
    "    dist = float((torch.nn.functional.cosine_similarity(embed1, embed2) + 1) / 2)\n",
    "    match = dist > threshold\n",
    "    \n",
    "    if return_dist:\n",
    "        return match, dist\n",
    "    \n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6db84d23-6602-4c6b-8305-121c364a7dc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607ba472077d41a1a555827272b61ff9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Получим эмбединги и предсказания по идентичности изображений\n",
    "# Я успела перебрать 5 трешхолдов и 0.75 дал лучший результат по f1-score для данной модели\n",
    "preds = np.array([[0] * len(pairs)] * 5)\n",
    "thresholds = [0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "\n",
    "for i, (idx1, idx2) in tqdm(enumerate(pairs)):\n",
    "    _, dist = is_match(val_dataset, idx1, idx2, return_dist=True)\n",
    "    \n",
    "    # Переберем в цикле условия по порогам, чтобы не гонять 5 раз сетку для одной и той же пары\n",
    "    for j in range(len(thresholds)):\n",
    "        if dist > thresholds[j]:\n",
    "            preds[j][i] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "4d9b12a8-4ba4-43bc-8f95-8933fde24245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Порог: 0.7\n",
      "    accuracy_score: 0.912\n",
      "    precision_score: 0.9090909090909091\n",
      "    recall_score: 0.9508196721311475\n",
      "    f1_score: 0.9294871794871795\n",
      "Порог: 0.75\n",
      "    accuracy_score: 0.926\n",
      "    precision_score: 1.0\n",
      "    recall_score: 0.8786885245901639\n",
      "    f1_score: 0.9354275741710296\n",
      "Порог: 0.8\n",
      "    accuracy_score: 0.882\n",
      "    precision_score: 1.0\n",
      "    recall_score: 0.8065573770491803\n",
      "    f1_score: 0.8929219600725953\n",
      "Порог: 0.85\n",
      "    accuracy_score: 0.778\n",
      "    precision_score: 1.0\n",
      "    recall_score: 0.6360655737704918\n",
      "    f1_score: 0.7775551102204408\n",
      "Порог: 0.9\n",
      "    accuracy_score: 0.606\n",
      "    precision_score: 1.0\n",
      "    recall_score: 0.3540983606557377\n",
      "    f1_score: 0.523002421307506\n"
     ]
    }
   ],
   "source": [
    "# Целевые метрики\n",
    "metrics = [accuracy_score, precision_score, recall_score, f1_score]\n",
    "thresholds = [0.7, 0.75, 0.8, 0.85, 0.9]\n",
    "for pred, thr in zip(preds, thresholds):\n",
    "    print(f\"Порог: {thr}\")\n",
    "    for m in metrics:\n",
    "        print(f\"    {m.__name__}: {m(targets, pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636729e6-d59d-420d-818c-d440987516be",
   "metadata": {},
   "source": [
    "# Построение knn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7e8d30-8ca3-448e-a922-1ab37d5af8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import v_measure_score\n",
    "import torchvision\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ca104a7-cdd0-4d81-8d2b-094ae20d2224",
   "metadata": {},
   "outputs": [],
   "source": [
    "trunk = torchvision.models.efficientnet_b3(weights=torchvision.models.EfficientNet_B3_Weights.DEFAULT)\n",
    "trunk_output_size = trunk.classifier[1].in_features\n",
    "trunk.classifier = nn.Identity()\n",
    "trunk = trunk.to(device)\n",
    "\n",
    "simple_embedder = nn.Sequential(nn.Linear(trunk_output_size, embedding_size))\n",
    "embedder = simple_embedder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06b0b604-143d-4cab-817f-8c0d68b900e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создадим экземляр инференса, по сути это просто обертка над двумя сетками, которая возвращает net2(net1(tensor))\n",
    "inference_model = InferenceModel(\n",
    "    trunk=trunk,\n",
    "    embedder=embedder,\n",
    "    normalize_embeddings=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bf999a30-147d-4f0e-add8-7d11139ad596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_model.train_knn(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1168b25a-bdf4-420b-9b90-58fc04db4fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.load_knn_func('knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ba2a6e4a-0e12-4421-8c30-6005f82755bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_model.save_knn_func('knn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0a55d48e-3658-4712-978d-8e36a47b73b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = DataLoader(val_dataset, batch_size=1, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2f84b4a7-2e95-4835-b823-60469158070e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = [\n",
    "    'test-task/clusters/003e4f79084c425eabb579482388820a.jpg',\n",
    "    'test-task/clusters/005f02dedd154c0889e84599e73f428f.jpg'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "06275a52-5226-4df7-af7f-aba773e1135c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lst' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_path \u001b[38;5;129;01min\u001b[39;00m \u001b[43mlst\u001b[49m:\n\u001b[1;32m      3\u001b[0m     image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(image_path)\n\u001b[1;32m      4\u001b[0m     image \u001b[38;5;241m=\u001b[39m get_valid_transforms()(image)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lst' is not defined"
     ]
    }
   ],
   "source": [
    "images = []\n",
    "for image_path in lst:\n",
    "    image = Image.open(image_path)\n",
    "    image = get_valid_transforms()(image)\n",
    "    images.append(image)\n",
    "\n",
    "batch = torch.stack(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8c1186d-f311-492e-be53-f069d7e019b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52b9c1ca-31b7-4a40-884d-5306f41e853a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected 4D input (got 3D input)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest-task/clusters/003e4f79084c425eabb579482388820a.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m get_valid_transforms()(image)\n\u001b[0;32m----> 4\u001b[0m dist, cl \u001b[38;5;241m=\u001b[39m \u001b[43minference_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_nearest_neighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/utils/inference.py:109\u001b[0m, in \u001b[0;36mInferenceModel.get_nearest_neighbors\u001b[0;34m(self, query, k)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nearest_neighbors\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, k):\n\u001b[0;32m--> 109\u001b[0m     query_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mknn_func(query_emb, k)\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/pytorch_metric_learning/utils/inference.py:119\u001b[0m, in \u001b[0;36mInferenceModel.get_embeddings\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 119\u001b[0m     x_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedder(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrunk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_embeddings:\n\u001b[1;32m    121\u001b[0m     x_emb \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(x_emb, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torchvision/models/efficientnet.py:343\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torchvision/models/efficientnet.py:333\u001b[0m, in \u001b[0;36mEfficientNet._forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 333\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mavgpool(x)\n\u001b[1;32m    336\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mflatten(x, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:138\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_input_dim\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# exponential_average_factor is set to self.momentum\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;66;03m# (when it is available) only so that it gets updated\u001b[39;00m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;66;03m# in ONNX graph when this node is exported to ONNX.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmomentum \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/python3.9/lib/python3.9/site-packages/torch/nn/modules/batchnorm.py:410\u001b[0m, in \u001b[0;36mBatchNorm2d._check_input_dim\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_input_dim\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    409\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected 4D input (got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124mD input)\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n",
      "\u001b[0;31mValueError\u001b[0m: expected 4D input (got 3D input)"
     ]
    }
   ],
   "source": [
    "image = Image.open('test-task/clusters/003e4f79084c425eabb579482388820a.jpg')\n",
    "image = get_valid_transforms()(image)\n",
    "\n",
    "dist, cl = inference_model.get_nearest_neighbors(image, k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3e993-63c3-442b-b4ae-ecbfe828ec93",
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "pred_labels = []\n",
    "for d, idx in zip(distances, indices):\n",
    "    dists.append(d[0])\n",
    "    pred_labels.append("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd28620-841e-4d85-9e8a-f4242af78d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cat(valid_labels_list, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "a52a9376-f666-4d0f-9fe5-ca7ab02fdc98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.6768],\n",
       "        [1.5984]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9911341a-d8bb-4c42-acde-24d78c34308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = get_valid_transforms()(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "193723c6-0ba7-4684-9808-cbb57433f189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1038)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cl[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6e46381-edc3-45c8-a68c-302c99bc8199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9dec2c1f-4605-4104-80a8-c49c168944c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'IndexFlat' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43minference_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mknn_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'IndexFlat' object is not callable"
     ]
    }
   ],
   "source": [
    "inference_model.knn_func.index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "05dd62e1-54d9-455a-b0c4-3090593dcd86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efa26ef53f4a4357a145ec1919892e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/305 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n",
      "INFO:PML:running k-nn with k=1\n",
      "INFO:PML:embedding dimensionality is 256\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "zero-dimensional tensor (at position 0) cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m     valid_indices_list\u001b[38;5;241m.\u001b[39mappend(indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     12\u001b[0m     preds_labels_list\u001b[38;5;241m.\u001b[39mappend(train_dataset\u001b[38;5;241m.\u001b[39mlabels[indices[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]])\n\u001b[0;32m---> 14\u001b[0m valid_labels \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalid_labels_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     15\u001b[0m valid_distances \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(valid_distance_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m     16\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(valid_indices_list, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: zero-dimensional tensor (at position 0) cannot be concatenated"
     ]
    }
   ],
   "source": [
    "valid_labels_list = []\n",
    "valid_distance_list = []\n",
    "valid_indices_list = []\n",
    "preds_labels_list = []\n",
    "\n",
    "for images, labels in tqdm(val_dataloader):\n",
    "\n",
    "    distances, indices = inference_model.get_nearest_neighbors(images, k=1)\n",
    "    valid_labels_list.append(labels[0])\n",
    "    valid_distance_list.append(distances[0])\n",
    "    valid_indices_list.append(indices[0][0])\n",
    "    preds_labels_list.append(train_dataset.labels[indices[0][0]])\n",
    "\n",
    "valid_labels = torch.cat(valid_labels_list, dim=0).numpy()\n",
    "valid_distances = torch.cat(valid_distance_list, dim=0).numpy()\n",
    "valid_indices = torch.cat(valid_indices_list, dim=0).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e04eb1ce-6522-4655-b060-d1e0fd7ffcf4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.labels[672]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a68cc31b-2737-41aa-a2b8-452093ddccae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# valid_labels_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a83b03e9-7fe0-44e6-b514-a7c0b587e532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4a611154-4124-4783-9514-ffb77fb0caa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22, 25,  7, 25,  2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ca953633-5466-4a89-bd32-ad839492ce94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_dataset.image_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7ffcacab-c27a-44e2-842d-14b86e45c4f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.iloc[672].label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0bf73bac-a2c2-4585-8608-6e5bdd0e4467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 672],\n",
       "       [1042],\n",
       "       [ 407],\n",
       "       [1176],\n",
       "       [ 717]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_indices[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dc4e12f2-bd1e-4aed-95e9-617534268f30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 2, 4, 7, 8, 12, 14, 15, 19, 24, 25}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(train_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "616a34dd-fb42-4d0f-aa17-ceef56e88465",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 3, 5, 6, 9, 10, 11, 13, 16, 17, 18, 20, 21, 22, 23}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(val_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1b2395bb-706b-45ff-806a-d0425549d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f657599a-a404-4093-9959-472a6afbcf8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_measure_score(['a', 'a'], ['a', 'a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156da9e-0f71-4a1b-964e-348e98bf7347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21d46529-ea27-4134-b10b-c6468889af15",
   "metadata": {},
   "source": [
    "## Целевая функция\n",
    "\n",
    "В связи с тем, что мы в формате jupyter notebook, я не буду здесь переподгружать все библиотеки и переменные, однако просто отмечу, что в прод-варианте, конечно, инферерс и трейн разнесены, поэтому все важные зависимости, переменные и веса необходимо\n",
    "- фиксировать в requirments\n",
    "- параметры и стурктуру модели выносить в файл, например, в YAML\n",
    "- параметры датасета, пути и варианты сэмплирования выносить в файл\n",
    "- иметь строгое логирование результатов экспериментов\n",
    "- визуализацию и историчность, например, в mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4552abb-f550-4700-bc98-5fe482f9bd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision, functional, torch\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a440b36-6188-4bb0-a40c-1c78a7a281d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trunk = torchvision.models.efficientnet_b3(weights=torchvision.models.EfficientNet_B3_Weights.DEFAULT)\n",
    "trunk_output_size = trunk.classifier[1].in_features\n",
    "trunk.classifier = torch.nn.Identity()\n",
    "trunk = trunk.to(device)\n",
    "\n",
    "simple_embedder = torch.nn.Sequential(torch.nn.Linear(trunk_output_size, 256))\n",
    "embedder = simple_embedder.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20bf65fe-5f9f-4b08-80e4-e7ec6ced1df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trunk_weights = torch.load('saved_models/trunk_10.pth', map_location=torch.device('cpu'))\n",
    "trunk.load_state_dict(trunk_weights)\n",
    "\n",
    "embedder_weights = torch.load('saved_models/embedder_10.pth', map_location=torch.device('cpu'))\n",
    "embedder.load_state_dict(embedder_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "10752de8-bb08-4fce-81ca-f2599af5c9c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tools'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[92], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_samples_from_class\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tools'"
     ]
    }
   ],
   "source": [
    "from tools.image_tools import get_samples_from_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "13475253-0651-40fe-b971-3003e1098398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/julia/Documents/interview/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5ac15b85-4896-45f7-b309-180984289235",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/julia/Documents/interview/',\n",
       " '/Users/julia/Documents/interview/tools/',\n",
       " './tools',\n",
       " 'tools',\n",
       " '/Users/julia/Documents/interview',\n",
       " '/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python39.zip',\n",
       " '/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9',\n",
       " '/usr/local/Cellar/python@3.9/3.9.10/Frameworks/Python.framework/Versions/3.9/lib/python3.9/lib-dynload',\n",
       " '',\n",
       " '/Users/julia/venvs/python3.9/lib/python3.9/site-packages',\n",
       " '../',\n",
       " '.',\n",
       " '/Users/julia/Documents/interview/']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac3b7bc2-11a8-44ba-acfc-7c579da08430",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ImageDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtools\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/interview/InteractiveStandard_testcase/tools/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage_tools\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m~/Documents/interview/InteractiveStandard_testcase/tools/image_tools.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_samples_from_class\u001b[39m(dataset: \u001b[43mImageDataset\u001b[49m, idx: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      4\u001b[0m     images \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mlabel_to_images[idx]\n\u001b[1;32m      5\u001b[0m     image_path \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mimage_dir\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ImageDataset' is not defined"
     ]
    }
   ],
   "source": [
    "import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdafaf2d-2ee6-498b-8249-173cf72824ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/julia/Documents/interview/InteractiveStandard_testcase\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08dd737-2f27-43cd-8872-5c8d1dbef89b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
